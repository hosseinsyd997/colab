{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP4Sc1QhTiGLRBy3Azi4kqz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hosseinsyd997/colab/blob/main/object_detection_model_trainer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1NWrZ6VHTXO_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f150fe09-f46b-4e14-f28b-80865518f507"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files copied successfully!\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Unzip the model.zip file\n",
        "with zipfile.ZipFile('model.zip', 'r') as zip_ref:\n",
        "    zip_ref.extractall('extracted_model')\n",
        "\n",
        "# Copy the contents of the model folder to the root directory\n",
        "source_dir = 'extracted_model/model'\n",
        "target_dir = './Data'\n",
        "\n",
        "for item in os.listdir(source_dir):\n",
        "    s = os.path.join(source_dir, item)\n",
        "    d = os.path.join(target_dir, item)\n",
        "    if os.path.isdir(s):\n",
        "        shutil.copytree(s, d, dirs_exist_ok=True)\n",
        "    else:\n",
        "        shutil.copy2(s, d)\n",
        "\n",
        "print(\"Files copied successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b955d91",
        "outputId": "77932715-227e-46e5-dd89-ef7161836624"
      },
      "source": [
        "!pip install tensorflowjs\n",
        "\n",
        "import os\n",
        "import xml.etree.ElementTree as ET\n",
        "import tensorflow as tf\n",
        "import tensorflowjs as tfjs\n",
        "import pandas as pd\n",
        "\n",
        "# Define paths\n",
        "image_dir = 'Data/images'\n",
        "annotation_dir = 'Data/annotations'\n",
        "\n",
        "# Function to parse XML files\n",
        "def parse_xml(xml_file):\n",
        "    tree = ET.parse(xml_file)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    image_info = {\n",
        "        'image_path': os.path.join(image_dir, root.find('filename').text),\n",
        "        'width': int(root.find('size').find('width').text),\n",
        "        'height': int(root.find('size').find('height').text),\n",
        "        'objects': []\n",
        "    }\n",
        "\n",
        "    for obj in root.findall('object'):\n",
        "        obj_info = {\n",
        "            'name': obj.find('name').text,\n",
        "            'xmin': int(obj.find('bndbox').find('xmin').text),\n",
        "            'ymin': int(obj.find('bndbox').find('ymin').text),\n",
        "            'xmax': int(obj.find('bndbox').find('xmax').text),\n",
        "            'ymax': int(obj.find('bndbox').find('ymax').text)\n",
        "        }\n",
        "        image_info['objects'].append(obj_info)\n",
        "\n",
        "    return image_info\n",
        "\n",
        "# Iterate through annotation files and parse them\n",
        "annotations = []\n",
        "for xml_file in os.listdir(annotation_dir):\n",
        "    if xml_file.endswith('.xml'):\n",
        "        annotations.append(parse_xml(os.path.join(annotation_dir, xml_file)))\n",
        "\n",
        "# Display a sample of the parsed data\n",
        "for i in range(2):\n",
        "    print(annotations[i])\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflowjs in /usr/local/lib/python3.11/dist-packages (4.22.0)\n",
            "Requirement already satisfied: flax>=0.7.2 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.10.6)\n",
            "Requirement already satisfied: importlib_resources>=5.9.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (6.5.2)\n",
            "Requirement already satisfied: jax>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.5.2)\n",
            "Requirement already satisfied: jaxlib>=0.4.13 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.5.1)\n",
            "Requirement already satisfied: tensorflow<3,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (2.18.0)\n",
            "Requirement already satisfied: tf-keras>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-decision-forests>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (1.11.0)\n",
            "Requirement already satisfied: six<2,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (1.17.0)\n",
            "Requirement already satisfied: tensorflow-hub>=0.16.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (0.16.1)\n",
            "Requirement already satisfied: packaging~=23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflowjs) (23.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (2.0.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (1.1.1)\n",
            "Requirement already satisfied: optax in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.2.5)\n",
            "Requirement already satisfied: orbax-checkpoint in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.11.19)\n",
            "Requirement already satisfied: tensorstore in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.74)\n",
            "Requirement already satisfied: rich>=11.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (13.9.4)\n",
            "Requirement already satisfied: typing_extensions>=4.2 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (4.14.1)\n",
            "Requirement already satisfied: PyYAML>=5.4.1 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (6.0.2)\n",
            "Requirement already satisfied: treescope>=0.1.7 in /usr/local/lib/python3.11/dist-packages (from flax>=0.7.2->tensorflowjs) (0.1.9)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (0.4.1)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax>=0.4.13->tensorflowjs) (1.16.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (18.1.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (75.2.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (1.74.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (3.14.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow<3,>=2.13.0->tensorflowjs) (0.37.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.45.1)\n",
            "Requirement already satisfied: wurlitzer in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (3.1.1)\n",
            "Requirement already satisfied: ydf in /usr/local/lib/python3.11/dist-packages (from tensorflow-decision-forests>=1.5.0->tensorflowjs) (0.13.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow<3,>=2.13.0->tensorflowjs) (0.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow<3,>=2.13.0->tensorflowjs) (2025.7.14)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=11.1->flax>=0.7.2->tensorflowjs) (2.19.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs) (3.8.2)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs) (3.1.3)\n",
            "Requirement already satisfied: chex>=0.1.87 in /usr/local/lib/python3.11/dist-packages (from optax->flax>=0.7.2->tensorflowjs) (0.1.90)\n",
            "Requirement already satisfied: etils[epath,epy] in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.13.0)\n",
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (1.6.0)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (4.12.3)\n",
            "Requirement already satisfied: simplejson>=3.16.0 in /usr/local/lib/python3.11/dist-packages (from orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.20.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->tensorflow-decision-forests>=1.5.0->tensorflowjs) (2025.2)\n",
            "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from chex>=0.1.87->optax->flax>=0.7.2->tensorflowjs) (0.12.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1->flax>=0.7.2->tensorflowjs) (0.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow<3,>=2.13.0->tensorflowjs) (3.0.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (2025.3.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.11/dist-packages (from etils[epath,epy]->orbax-checkpoint->flax>=0.7.2->tensorflowjs) (3.23.0)\n",
            "{'image_path': 'Data/images/70c171cc-photo_7_2025-07-27_04-38-43.jpg', 'width': 1280, 'height': 1001, 'objects': [{'name': 'RP', 'xmin': 743, 'ymin': 348, 'xmax': 753, 'ymax': 355}, {'name': 'RP', 'xmin': 887, 'ymin': 361, 'xmax': 893, 'ymax': 367}]}\n",
            "{'image_path': 'Data/images/85b0aaa2-photo_6_2025-07-27_04-38-43.jpg', 'width': 1280, 'height': 1001, 'objects': [{'name': 'RP', 'xmin': 539, 'ymin': 323, 'xmax': 545, 'ymax': 325}, {'name': 'RP', 'xmin': 714, 'ymin': 318, 'xmax': 723, 'ymax': 327}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "13Wb_JErUYAq",
        "outputId": "d8183a87-233b-4610-f229-094a6362a569"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    image = image / 255.0  # Normalize to [0,1]\n",
        "    return image\n",
        "\n",
        "# Get all unique labels\n",
        "all_labels = [obj['name'] for ann in annotations for obj in ann['objects']]\n",
        "unique_labels = sorted(list(set(all_labels)))\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "num_classes = len(unique_labels)\n",
        "\n",
        "def create_dataset(annotations):\n",
        "    image_paths = []\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    for ann in annotations:\n",
        "        if not ann['objects']:\n",
        "            continue\n",
        "\n",
        "        image_paths.append(ann['image_path'])\n",
        "\n",
        "        # Take only the first object for simplicity\n",
        "        obj = ann['objects'][0]\n",
        "\n",
        "        # Normalize bounding box coordinates\n",
        "        xmin = obj['xmin'] / ann['width']\n",
        "        ymin = obj['ymin'] / ann['height']\n",
        "        xmax = obj['xmax'] / ann['width']\n",
        "        ymax = obj['ymax'] / ann['height']\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label_to_id[obj['name']])\n",
        "\n",
        "    # Create a TensorFlow dataset\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    box_ds = tf.data.Dataset.from_tensor_slices(tf.constant(boxes, dtype=tf.float32))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(tf.constant(labels, dtype=tf.int64)).map(lambda x: tf.one_hot(x, depth=num_classes))\n",
        "    label_ds = label_ds.map(lambda x: tf.squeeze(x, axis=0))\n",
        "\n",
        "\n",
        "    # Zip the datasets together\n",
        "    dataset = tf.data.Dataset.zip((image_ds, (box_ds, label_ds)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Create the final dataset\n",
        "dataset = create_dataset(annotations)\n",
        "\n",
        "# Print the element spec of the dataset to verify its structure\n",
        "print(dataset.element_spec)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(128, 128, 3), dtype=tf.float32, name=None), (TensorSpec(shape=(4,), dtype=tf.float32, name=None), TensorSpec(shape=(), dtype=tf.float32, name=None)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncTAON8JUd7m",
        "outputId": "7db27a2b-2b7b-4703-c8ea-116ad6821451"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def load_and_preprocess_image(path):\n",
        "    image = tf.io.read_file(path)\n",
        "    image = tf.image.decode_jpeg(image, channels=3)\n",
        "    image = tf.image.resize(image, [128, 128])\n",
        "    image = image / 255.0  # Normalize to [0,1]\n",
        "    return image\n",
        "\n",
        "# Get all unique labels\n",
        "all_labels = [obj['name'] for ann in annotations for obj in ann['objects']]\n",
        "unique_labels = sorted(list(set(all_labels)))\n",
        "label_to_id = {label: i for i, label in enumerate(unique_labels)}\n",
        "num_classes = len(unique_labels)\n",
        "\n",
        "def create_dataset(annotations):\n",
        "    image_paths = []\n",
        "    boxes = []\n",
        "    labels = []\n",
        "\n",
        "    for ann in annotations:\n",
        "        if not ann['objects']:\n",
        "            continue\n",
        "\n",
        "        image_paths.append(ann['image_path'])\n",
        "\n",
        "        # Take only the first object for simplicity\n",
        "        obj = ann['objects'][0]\n",
        "\n",
        "        # Normalize bounding box coordinates\n",
        "        xmin = obj['xmin'] / ann['width']\n",
        "        ymin = obj['ymin'] / ann['height']\n",
        "        xmax = obj['xmax'] / ann['width']\n",
        "        ymax = obj['ymax'] / ann['height']\n",
        "        boxes.append([xmin, ymin, xmax, ymax])\n",
        "        labels.append(label_to_id[obj['name']])\n",
        "\n",
        "    # Create a TensorFlow dataset\n",
        "    path_ds = tf.data.Dataset.from_tensor_slices(image_paths)\n",
        "    image_ds = path_ds.map(load_and_preprocess_image, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "    box_ds = tf.data.Dataset.from_tensor_slices(tf.constant(boxes, dtype=tf.float32))\n",
        "    label_ds = tf.data.Dataset.from_tensor_slices(tf.constant(labels, dtype=tf.int64))\n",
        "\n",
        "    # One-hot encode the labels. The model's loss function for classification is BinaryCrossentropy,\n",
        "    # which expects a single value per image since we have one output neuron.\n",
        "    # If num_classes > 1, you would use tf.one_hot and CategoricalCrossentropy.\n",
        "    if num_classes == 1:\n",
        "        label_ds = label_ds.map(lambda x: tf.cast(x, tf.float32))\n",
        "        label_ds = label_ds.map(lambda x: tf.expand_dims(x, axis=-1)) # Reshape to (1,)\n",
        "    else:\n",
        "        label_ds = label_ds.map(lambda x: tf.one_hot(x, depth=num_classes))\n",
        "\n",
        "\n",
        "    # Zip the datasets together\n",
        "    dataset = tf.data.Dataset.zip((image_ds, (box_ds, label_ds)))\n",
        "\n",
        "    return dataset\n",
        "\n",
        "# Create the final dataset\n",
        "dataset = create_dataset(annotations)\n",
        "\n",
        "# Print the element spec of the dataset to verify its structure\n",
        "print(dataset.element_spec)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(TensorSpec(shape=(128, 128, 3), dtype=tf.float32, name=None), (TensorSpec(shape=(4,), dtype=tf.float32, name=None), TensorSpec(shape=(1,), dtype=tf.float32, name=None)))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f589dc4",
        "outputId": "c1c52bb4-08b4-4493-9ce7-322ea2774fb9"
      },
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import os\n",
        "\n",
        "def parse_annotation(xml_file_path):\n",
        "    tree = ET.parse(xml_file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    image_path = root.find('path').text if root.find('path') is not None else None\n",
        "    width_elem = root.find('size/width')\n",
        "    height_elem = root.find('size/height')\n",
        "\n",
        "    if width_elem is None or height_elem is None:\n",
        "        return None\n",
        "\n",
        "    width = int(width_elem.text)\n",
        "    height = int(height_elem.text)\n",
        "\n",
        "    objects = []\n",
        "    for obj in root.findall('object'):\n",
        "        name_elem = obj.find('name')\n",
        "        bndbox = obj.find('bndbox')\n",
        "        if bndbox is not None and name_elem is not None:\n",
        "            xmin_elem = bndbox.find('xmin')\n",
        "            ymin_elem = bndbox.find('ymin')\n",
        "            xmax_elem = bndbox.find('xmax')\n",
        "            ymax_elem = bndbox.find('ymax')\n",
        "\n",
        "            if all(elem is not None for elem in [xmin_elem, ymin_elem, xmax_elem, ymax_elem]):\n",
        "                objects.append({\n",
        "                    'name': name_elem.text,\n",
        "                    'xmin': int(xmin_elem.text),\n",
        "                    'ymin': int(ymin_elem.text),\n",
        "                    'xmax': int(xmax_elem.text),\n",
        "                    'ymax': int(ymax_elem.text)\n",
        "                })\n",
        "\n",
        "    # If image_path is not directly available, construct it based on the filename\n",
        "    if image_path is None and root.find('filename') is not None:\n",
        "        filename = root.find('filename').text\n",
        "        image_path = os.path.join('Data/images', filename)\n",
        "\n",
        "    if not image_path or not objects:\n",
        "        return None\n",
        "\n",
        "\n",
        "    return {\n",
        "        'image_path': image_path,\n",
        "        'width': width,\n",
        "        'height': height,\n",
        "        'objects': objects\n",
        "    }\n",
        "\n",
        "annotations = []\n",
        "for filename in os.listdir('Data/annotations'):\n",
        "    if filename.endswith('.xml'):\n",
        "        xml_path = os.path.join('Data/annotations', filename)\n",
        "        annotation_data = parse_annotation(xml_path)\n",
        "        if annotation_data:\n",
        "            annotations.append(annotation_data)\n",
        "\n",
        "if annotations:\n",
        "    print(annotations[0])\n",
        "else:\n",
        "    print(\"No annotations found.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'image_path': 'Data/images/70c171cc-photo_7_2025-07-27_04-38-43.jpg', 'width': 1280, 'height': 1001, 'objects': [{'name': 'RP', 'xmin': 743, 'ymin': 348, 'xmax': 753, 'ymax': 355}, {'name': 'RP', 'xmin': 887, 'ymin': 361, 'xmax': 893, 'ymax': 367}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ce2b2cd",
        "outputId": "ba55a836-4300-4a9e-eb83-24b9beda7452"
      },
      "source": [
        "from tensorflow.keras.applications import MobileNetV2\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "def build_model(num_classes):\n",
        "    base_model = MobileNetV2(input_shape=(128, 128, 3), include_top=False, weights='imagenet')\n",
        "    base_model.trainable = False  # Freeze the base model\n",
        "\n",
        "    inputs = Input(shape=(128, 128, 3))\n",
        "    x = base_model(inputs, training=False)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    # Bounding box regression head\n",
        "    box_head = Dense(128, activation='relu')(x)\n",
        "    box_head = Dense(64, activation='relu')(box_head)\n",
        "    box_head = Dense(4, activation='sigmoid', name='box_head')(box_head)\n",
        "\n",
        "    # Classification head\n",
        "    class_head = Dense(128, activation='relu')(x)\n",
        "    class_head = Dense(64, activation='relu')(class_head)\n",
        "    # For binary classification (or single class), a single output neuron is sufficient.\n",
        "    # If you have more than one class, change the number of units to num_classes\n",
        "    # and the activation to 'softmax'.\n",
        "    class_head = Dense(1 if num_classes == 1 else num_classes, activation='sigmoid' if num_classes == 1 else 'softmax', name='class_head')(class_head)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=[box_head, class_head])\n",
        "    return model\n",
        "\n",
        "model = build_model(num_classes)\n",
        "\n",
        "# Define losses and optimizer\n",
        "losses = {\n",
        "    \"box_head\": tf.keras.losses.MeanSquaredError(),\n",
        "    \"class_head\": tf.keras.losses.BinaryCrossentropy() if num_classes == 1 else tf.keras.losses.CategoricalCrossentropy()\n",
        "}\n",
        "loss_weights = {\"box_head\": 1.0, \"class_head\": 1.0}\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer=optimizer, loss=losses, loss_weights=loss_weights, metrics={\"class_head\": \"accuracy\"})\n",
        "\n",
        "# Prepare the dataset for training\n",
        "def prepare_for_training(ds, batch_size=4, shuffle_buffer_size=100):\n",
        "    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
        "    ds = ds.repeat()\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_dataset = prepare_for_training(dataset)\n",
        "\n",
        "# Train the model\n",
        "# We need to filter out annotations with no objects before calculating the number of steps\n",
        "annotations_with_objects = [ann for ann in annotations if ann['objects']]\n",
        "history = model.fit(train_dataset, epochs=10, steps_per_epoch=len(annotations_with_objects) // 4)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 73ms/step - box_head_loss: 0.0648 - class_head_accuracy: 0.6667 - class_head_loss: 0.4903 - loss: 0.5551\n",
            "Epoch 2/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step - box_head_loss: 0.0317 - class_head_accuracy: 1.0000 - class_head_loss: 0.0027 - loss: 0.0344\n",
            "Epoch 3/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - box_head_loss: 0.0103 - class_head_accuracy: 1.0000 - class_head_loss: 2.0468e-04 - loss: 0.0105\n",
            "Epoch 4/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - box_head_loss: 0.0143 - class_head_accuracy: 1.0000 - class_head_loss: 6.9851e-06 - loss: 0.0144\n",
            "Epoch 5/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 115ms/step - box_head_loss: 0.0054 - class_head_accuracy: 1.0000 - class_head_loss: 1.1784e-06 - loss: 0.0054\n",
            "Epoch 6/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step - box_head_loss: 0.0085 - class_head_accuracy: 1.0000 - class_head_loss: 2.2975e-07 - loss: 0.0085\n",
            "Epoch 7/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 102ms/step - box_head_loss: 0.0116 - class_head_accuracy: 1.0000 - class_head_loss: 2.7116e-08 - loss: 0.0116\n",
            "Epoch 8/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 116ms/step - box_head_loss: 0.0025 - class_head_accuracy: 1.0000 - class_head_loss: 2.6763e-08 - loss: 0.0025\n",
            "Epoch 9/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 131ms/step - box_head_loss: 0.0055 - class_head_accuracy: 1.0000 - class_head_loss: 1.1093e-09 - loss: 0.0055\n",
            "Epoch 10/10\n",
            "\u001b[1m2/2\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step - box_head_loss: 0.0051 - class_head_accuracy: 1.0000 - class_head_loss: 7.4481e-09 - loss: 0.0051\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9ce6ff3",
        "outputId": "e97fd45e-375c-4cd8-ec54-3b2f9eec0216"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Save the model in the SavedModel format\n",
        "tf.saved_model.save(model, 'saved_model')\n",
        "\n",
        "# Convert the SavedModel to TensorFlow.js format\n",
        "!tensorflowjs_converter --input_format=tf_saved_model saved_model tfjs_model"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-07-30 04:24:29.642953: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1753849469.681556    3925 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1753849469.693541    3925 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[32mðŸŒ² Try \u001b[0m\u001b[34mhttps://ydf.readthedocs.io\u001b[0m\u001b[32m, the successor of TensorFlow Decision Forests with more features and faster training!\u001b[0m\n",
            "2025-07-30 04:24:35.726277: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "I0000 00:00:1753849477.330548    3925 devices.cc:67] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 0\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1753849477.330801    3925 single_machine.cc:361] Starting new session\n"
          ]
        }
      ]
    }
  ]
}